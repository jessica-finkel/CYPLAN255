{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40ef5a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CYPLAN255\n",
    "### Urban Informatics and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc2c5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "HIT RECORD and TRANSCRIBE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceea7a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 08 -- Data Analysis\n",
    "*******\n",
    "February 16, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7d376",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "1. Announcements\n",
    "2. Review of last session\n",
    "3. Data Analysis\n",
    "4. For next time\n",
    "5. Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c0cef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Announcements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d336f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Assignment 2 due _Sunday_\n",
    "2. Update to GitHub guide\n",
    "3. Dante and Virgil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221723a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.3 Dante and Virgil\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/70/Eug%C3%A8ne_Delacroix_-_The_Barque_of_Dante.jpg\" width=50%>\n",
    "\n",
    "_The Barque of Dante_, Eug√®ne Delacroix (1822)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b61f31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Review from Intro to Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a93919",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 2.1 Indexing and Selecting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4783c11d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- use `[<row>, <column>]` indexing with `loc` (labels) and `iloc` (positions)\n",
    "- use `df[<col>]` to select a column/Series from a DataFrame by column label\n",
    "- use `df[[<col1>, <col2>]]` to select a subset of a DataFrame by column label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4722755",
   "metadata": {},
   "source": [
    "JFF:\n",
    "- we can use row, column indexing\n",
    "- we can use the \"loc\" /lohk/ operator to index with labels or \"iloc\" /eye lohk/ to index positionally\n",
    "- we can use a dataframe with brackets (df[]) to select a subset of a dataframe by a column label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bc62c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2 Merge Methods\n",
    "\n",
    "Most of the below is taken directly from the pandas [docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#brief-primer-on-merge-methods-relational-algebra).\n",
    "\n",
    "JFF -- see the pandas docs/user guide -- overlaps with the McKinney book (Python for Data Analysis), but extremely helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45af0e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are 4 main types of merges, but all of them require **four** arguments (i.e., have four core arguments/elements):\n",
    "- `left`: first `DataFrame` or `Series` to merge  (a left table you are merging)\n",
    "- `right`: second `DataFrame` or `Series` to merge with the first (a right table you are merging)\n",
    "- `on`: the name of the \"key\" column(s) you'll use to identify matching/corresponding rows in each table (a key/column you are merging on)\n",
    "- `how`: the type of merge to perform (default is \"inner\") (instructions for how to perform the merge, e.g,. inner vs. outer, left, right, join, etc. The default is that python will do an inner join.)\n",
    "\n",
    "A left join compares the right table to the left table and preserves the LEFT (in code, the table you are preserving comes first)\n",
    "\n",
    "A right join compares the left to the right and preserves the RIGHT.\n",
    "\n",
    "An outer join is a full join -- won't drop any rows\n",
    "\n",
    "A cross join is a Cartesian product -- we won't use them in class, but worth knowing about.\n",
    "\n",
    "When you preserve values that don't have a match, you get a null or a naan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a53b90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how this works in practice. First we'll create some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e09938",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "left = pd.DataFrame(\n",
    "\n",
    "    {\n",
    "        \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"],\n",
    "        \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "    }\n",
    "\n",
    ")\n",
    "\n",
    "right = pd.DataFrame(\n",
    "    {\n",
    "        \"key1\": [\"K0\", \"K1\", \"K1\", \"K2\"],\n",
    "        \"key2\": [\"K0\", \"K0\", \"K0\", \"K0\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6656756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (left)\n",
    "print (right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a0695",
   "metadata": {},
   "source": [
    "And now we'll do some merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea27005f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**inner** join\n",
    "![](https://pandas.pydata.org/pandas-docs/stable/_images/merging_merge_on_key_multiple.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca7931",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(left, right, on=[\"key1\", \"key2\"])  # inner is default \"how\", so we don't need to specify it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d322aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**left** join\n",
    "![](https://pandas.pydata.org/pandas-docs/stable/_images/merging_merge_on_key_left.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9047366f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(left, right, how=\"left\", on=[\"key1\", \"key2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a175a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**right** join\n",
    "![](https://pandas.pydata.org/pandas-docs/stable/_images/merging_merge_on_key_right.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f241bae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(right, left, how=\"left\", on=[\"key1\", \"key2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4b6a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**outer** (A.K.A. full) join\n",
    "![](https://pandas.pydata.org/pandas-docs/stable/_images/merging_merge_on_key_outer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe044ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(left, right, how=\"outer\", on=[\"key1\", \"key2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209cc05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**cross** join (A.K.A. Cartesian Product)\n",
    "![](https://pandas.pydata.org/pandas-docs/stable/_images/merging_merge_cross.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70f3c0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(left, right, how=\"cross\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd29dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.3 Handling duplicate column names\n",
    "\n",
    "(how to handle duplicate column names if you arent' trying to merge on a column that shows up in both tables (because pandas assumes you want to merge on any column that is in both tables, so if you don't tell it what to do with the ones in both that you are not merging on, it will create a duplicate ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff9174",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "left = pd.DataFrame({\"A\": [4, 5], \"B\": [2, 2], \"C\": [5, 7]})\n",
    "right = pd.DataFrame({\"A\": [4, 5, 6], \"B\": [2, 2, 2], \"D\": [9, 8, 1]})\n",
    "pd.merge(left, right, on=[\"B\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e6fce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3.1 Define your own suffixes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c00fec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Keep all columns, but give them meaningful names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881400a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(left, right, on=\"B\", how=\"inner\", suffixes=('_left', '_right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ead92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Use suffixes to drop duplicate columns after merge**\n",
    "\n",
    "JFF -- use suffixes to drop duplicate columns after merge or use suffixes to create a list of columns that you don't want and then pass them to dataframe drop method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f771a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, on=\"B\", how=\"inner\", suffixes=('_trash', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bcafe0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# keep only cols you want\n",
    "good_cols = [col for col in result.columns if not 'trash' in col]\n",
    "result[[\"B\", \"C\", \"A\", \"D\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef30a41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# drop cols you don't want\n",
    "trash_cols = [col for col in result.columns if 'trash' in col]\n",
    "result.drop(trash_cols, axis=1)  # \"axis\" tells pandas to drop columns (axis=1) or rows (axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691b7aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3.2 Filter out duplicate columns before you merge!\n",
    "\n",
    "JFF -- this way is a bit cleaner. First, identify the merge key column. The list comprehension below says that \"left_col_mask will be a list that contains all of the column names from the left table that do not appear in the right table, plus the key column/or are they key\"\n",
    "\n",
    "List comprehension is a for-loop that you do in one line in a list, rather than writing several lines of code with a for-loop and if/else instructions.\n",
    "\n",
    "With 2D arrays, have two axes, x = row = 0, y = column = 1 (think of it as because a line with a slope of 1 is a vertical line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fabb32",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "key = \"B\"\n",
    "left_col_mask = [\n",
    "    col for col in left.columns if (col not in right.columns) or (col == key)]\n",
    "pd.merge(left[left_col_mask], right, on=key, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9afbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Exploratory Data Analysis with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01815ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**READ THE DOCS**\n",
    "\n",
    "Most of the pandas-related material we saw **yesterday** corresponds to three sections of the pandas user guide:\n",
    "1. [Basics](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html)\n",
    "2. [Indexing and Selecting](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)\n",
    "3. [Merge, join, concatenate, and compare](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)\n",
    "\n",
    "\n",
    "Most of the material we're covering **today** is also covered in the following three sections of the pandas user guide:\n",
    "1. [Group by: split-apply-combine](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html)\n",
    "2. [Reshaping and pivot tables](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html)\n",
    "3. [Chart visualization](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html)\n",
    "\n",
    "**Everything** we're covering in pandas is summarized nicely in the [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ec377",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 The `groupby()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f8246b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the first part of this lesson, we're going to be analyzing some Bay Area Census data which I've already grabbed for you (specifically the Summary File 1 (SF1) data from the 2010 Census). Refer to the [SF1 Data Dictionary](https://www.census.gov/prod/cen2010/doc/sf1.pdf) to see the list of column codes. I've compiled this data into an HDF5 file, which is kind of like a .zip archive that lets you unzip one file at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80104220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using head here to show just a sample of rows because it's a huge data sample\n",
    "\n",
    "sf1 = pd.read_hdf('data/bay_sf1_small.h5', 'sf1_extract')\n",
    "sf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897d0c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note on HDF5 dependencies**\n",
    "\n",
    "pandas uses the `pytables` library to interact with .h5 files. If you don't have it installed and you try to run the cell above, Python may complain. Fear not. Did you know you can execute bash terminal commands directly from a notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa082556",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logrecno</th>\n",
       "      <th>blockfips</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>tract</th>\n",
       "      <th>blkgrp</th>\n",
       "      <th>block</th>\n",
       "      <th>arealand</th>\n",
       "      <th>P0010001</th>\n",
       "      <th>P0020001</th>\n",
       "      <th>...</th>\n",
       "      <th>H0040003</th>\n",
       "      <th>H0040004</th>\n",
       "      <th>H0050001</th>\n",
       "      <th>H0050002</th>\n",
       "      <th>H0050003</th>\n",
       "      <th>H0050004</th>\n",
       "      <th>H0050005</th>\n",
       "      <th>H0050006</th>\n",
       "      <th>H0050007</th>\n",
       "      <th>H0050008</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>060014271001000</td>\n",
       "      <td>06</td>\n",
       "      <td>001</td>\n",
       "      <td>427100</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>060014271001001</td>\n",
       "      <td>06</td>\n",
       "      <td>001</td>\n",
       "      <td>427100</td>\n",
       "      <td>1</td>\n",
       "      <td>1001</td>\n",
       "      <td>79696</td>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>060014271001002</td>\n",
       "      <td>06</td>\n",
       "      <td>001</td>\n",
       "      <td>427100</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>060014271001003</td>\n",
       "      <td>06</td>\n",
       "      <td>001</td>\n",
       "      <td>427100</td>\n",
       "      <td>1</td>\n",
       "      <td>1003</td>\n",
       "      <td>19546</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>060014271001004</td>\n",
       "      <td>06</td>\n",
       "      <td>001</td>\n",
       "      <td>427100</td>\n",
       "      <td>1</td>\n",
       "      <td>1004</td>\n",
       "      <td>14364</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   logrecno        blockfips state county   tract  blkgrp block  arealand  \\\n",
       "0        25  060014271001000    06    001  427100       1  1000         0   \n",
       "1        26  060014271001001    06    001  427100       1  1001     79696   \n",
       "2        27  060014271001002    06    001  427100       1  1002       739   \n",
       "3        28  060014271001003    06    001  427100       1  1003     19546   \n",
       "4        29  060014271001004    06    001  427100       1  1004     14364   \n",
       "\n",
       "   P0010001  P0020001  ...  H0040003  H0040004  H0050001  H0050002  H0050003  \\\n",
       "0         0         0  ...         0         0         0         0         0   \n",
       "1       113       113  ...         1         4         0         0         0   \n",
       "2         0         0  ...         0         0         0         0         0   \n",
       "3        29        29  ...         0         7         3         0         0   \n",
       "4        26        26  ...         0         6         0         0         0   \n",
       "\n",
       "   H0050004  H0050005  H0050006  H0050007  H0050008  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         1         0         0         0         2  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf1 = pd.read_hdf('data/bay_sf1_small.h5', 'sf1_extract')\n",
    "sf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13110b3e",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# !conda install pytables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed74a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1.1 Basic Data Transformations\n",
    "\n",
    "In the world of machine learning, data transformation is sometimes called **feature extraction**. Both terms refer to the process of taking \"raw\" input data and manipulating it to create new, useful data. In the example below, we convert population totals to percentages:\n",
    "\n",
    "\n",
    "JFF--\n",
    "Data transformations = feature extraction; both terms mean taking \"raw\" input data and manipulating it to create new, useful data.\n",
    "\n",
    "Through these methods we are creating new series from other series, and when we do that, pandas will append new series as columns on the right-hand side of the table. \n",
    "\n",
    "Below we are taking columns we already have and combining them to create new columns in the table. (If you don't want to append the new columns right away and just want to explore and see them first, you can save them as a new variable and append them later if you like them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbb937",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sf1['pct_black'] = sf1['P0030003'] / sf1['P0030001'] * 100\n",
    "sf1['pct_asian'] = sf1['P0030005'] / sf1['P0030001'] * 100\n",
    "sf1['pct_white'] = sf1['P0030002'] / sf1['P0030001'] * 100\n",
    "sf1['pct_hisp'] = sf1['P0040003'] / sf1['P0040001'] * 100\n",
    "sf1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970fa3a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now add colums with percentage rental and population per square mile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90637963",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sf1['pct_rent'] = sf1['H0040004'] / sf1['H0040001'] * 100\n",
    "sf1['pop_sqmi'] = (sf1['P0010001'] / (sf1['arealand'] / 2589988))\n",
    "sf1 = sf1[sf1['P0030001'] > 0]\n",
    "sf1.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541bdea1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice how when we create new columns they get automatically appended to the end (right) of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d0faf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's add county names to the dataframe so we get more readable output. First we'll create a dictionary to map the FIPS codes to county names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb53da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "county_fips_to_name = {\n",
    "    '001': 'Alameda', '013': 'Contra Costa', '041': 'Marin', '055': 'Napa', '075': 'San Francisco',\n",
    "    '081': 'San Mateo', '085': 'Santa Clara', '095': 'Solano', '097': 'Sonoma'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85167252",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then we pass that dictionary to the `pd.Series.replace()` method to perform the conversion.\n",
    "\n",
    "We could use the same assignment-based approach to create our new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79042f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sf1['county_name'] = sf1['county'].replace(county_fips_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff23bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "or we can use `pd.DataFrame.insert()` to tell pandas exactly where to stick the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1bd090",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "del sf1['county_name']  # drop the column we just created\n",
    "sf1.insert(4, 'county_name', sf1['county'].replace(county_fips_to_name))\n",
    "sf1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844ea65",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice how `insert()` operates \"in-place\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde65a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1.2 Split-Apply-Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b10c691",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Groupby is a powerful method in pandas that follows the split-apply-combine approach to data manipulation.\n",
    "\n",
    "<center><img src=\"images/groupby.png\" width=75%></center>\n",
    "\n",
    "source: Fig. 10-1 _Python for Data Analysis_, McKinney (2017)\n",
    "\n",
    "\n",
    "JFF -- Split-Apply-Combine Paradigm involves splitting data according to their key (i.e., grouping them), and then applying an operation before combining the data.\n",
    "\n",
    "Step 1: split - returns an object. (If you take the length of it, it'll tell you how many groups you have)\n",
    "STep 2: apply -- applies aggregation methods where for each group, it'll take a series of values and combine them to produce one value, like a min/mean/max. It'll tell you the position of the min, max, quantiles, percentiles, etc. Takes a bunch of numbers and condenses to a single number\n",
    "Step 3: combine -- to apply chosen aggregation, you call it directly as a method of your groupby object. The object that pandas returns will be the combined outputs of the methods for each of your groups.\n",
    "\n",
    "In practice, the three steps are done together/coded in one line using chained assignment, where we chain one method on top of another. Rather than storing each variable, passing it to sum, and then renaming that variable, etc., we do it in one line of code that chains it for you.\n",
    "\n",
    "Aggregating (REWATCH LECTURE HERE) ...very helpful and can be very powerful for data analysis, e.g., saying in one line \"do this with things in this column and do something else with things in that columm\"\n",
    "\n",
    "WATCH CRAIGSLIST SECTION OF LECTURE re: cleaning messy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d3d41b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SPLIT**\n",
    "\n",
    "Let's apply this approach to computing total population in each county in our dataset. First we create a groupby object, using county codes to group all the census blocks in sf1 into groups that share the same county code. This represents the **split** part of the workflow in the figure above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf8385",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grouped = sf1[['P0010001', 'county_name']].groupby('county_name')\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3a6aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**APPLY**\n",
    "\n",
    "Now were ready to apply an operation to each group we've split. We call these **aggregation** methods because for each group they will take a series of values and combine them to produce one value, like a min/max/mean. pandas provides a bunch of built-in aggregation functions for use with `groupby` object. Some of the most common ones include:\n",
    "\n",
    "* `count`\n",
    "* `sum`\n",
    "* `mean`\n",
    "* `median`\n",
    "* `std`, `var`\n",
    "* `min`, `max`\n",
    "* `idxmax`, `idxmin`\n",
    "* `first`, `last`\n",
    "* `quantile`\n",
    "\n",
    "But you can also define and apply your own functions to use for aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9edb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**COMBINE**\n",
    "\n",
    "To apply your chosen aggregation, you can call it directly as a method of your `groupby` object. The object pandas returns will be the **combined** outputs of this method for each of your groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577d6dd",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grouped.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a1262",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SPLIT-APPLY-COMBINE**\n",
    "\n",
    "Doing this in two steps like above is really just to clarify the two parts of the split and apply process that happen within a groupy operation. Normally we would not bother separately creating a groupby object -- we would just do this in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820460db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "county_pop = sf1[['county_name', 'P0010001']].rename(\n",
    "    columns={'P0010001': 'total_pop'}).groupby(\n",
    "    'county_name').sum()\n",
    "county_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455639c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2 Aggregating on multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c53a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's merge the county totals with the original sf1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29baa1",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sf2 = pd.merge(sf1, county_pop, left_on='county_name', right_index=True, how='inner')\n",
    "sf2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05622b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we wanted to compute the population per square mile by county.  We could go ahead and create another dataframe with total area by county than then divide the total population by total area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b448110",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "county_land = sf1[['county_name', 'arealand']].groupby(sf1['county_name']).sum()\n",
    "county_land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8e7f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sq_m_to_sq_mi = 2589988.11 \n",
    "county_pop['total_pop'] / county_land['arealand'] * sq_m_to_sq_mi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f68b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or, we could have done both aggregations at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93fc1d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sf1[['county_name', 'P0010001', 'arealand']].groupby('county_name').sum()\n",
    "# county_totals['pop_density'] = county_totals['P0010001'] / county_totals['arealand'] * sq_m_to_sq_mi\n",
    "# county_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23f687",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we want to apply different aggregations to different columns? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36fbad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sf1.groupby('county_name').agg({'pct_asian': 'mean', 'P0010001': 'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f04798",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question**\n",
    "\n",
    "Sometimes I write `df[<list of columns>].groupby()`, but sometimes I just do `df.groupby()`. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dfb7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.3 Exercises:\n",
    "\n",
    "Count the number of census blocks per county.\n",
    "\n",
    "Calculate total households per county.\n",
    "\n",
    "Calculate percent renters by county. (Careful not to calculate the mean percent rental across blocks in a county)\n",
    "\n",
    "Calculate percent vacant by county.\n",
    "\n",
    "Calculate mean, min and max pop_sqmi (at the block level) by county.\n",
    "\n",
    "Calculate the 90th percentile of pop_sqmi (at the block level) by county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cb9d4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of census blocks per county:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae86fba",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('Total households per county')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74de19",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('percent renters by county')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f9eab",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('Percent vacant by county')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594474e1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('Min, Max and Mean Population per SQMI by Census Block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83535ab",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('90th Percentile of Population per SQMI at block level by County')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e866bdcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.4 Cleaning Messy Data -- Craigslist Rental Listings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc2c27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.4.1 Loading data\n",
    "Let's load some rental listings I scraped from Craigslist.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfcd9f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/bay.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603679b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It turns out to be pretty messy. What problems do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427a5c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- Neighborhood names are in parentheses...\n",
    "- Bedrooms and Square Feet are embedded in a single string in the bedrooms column along with other text...\n",
    "- Price is formatted as a string with a dollar sign...\n",
    "- Date is a string in a non-standard format...\n",
    "\n",
    "So how can we go about cleaning these data up to use them for analysis?\n",
    "\n",
    "Let's start with cleaning up the Price and Neighborhood variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372b48c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.4.2 String Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7316842",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['price'] = df['price'].str.strip('$').astype('float64')\n",
    "df['neighborhood'] = df['neighborhood'].str.strip().str.strip('(').str.strip(')')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf863ada",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK, now lets create Year, Month and Day columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c8f16",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['month'] = df['date'].str.split().str[0]\n",
    "df['day'] = df['date'].str.split().str[1].astype('int32')\n",
    "df['year'] = df['date'].str.split().str[2].astype('int32')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76411b30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.4.3 Datetime ops\n",
    "\n",
    "pandas has special functions for dealing with `datetime` data types which make it much easier to do what we just did above. First we have to convert our date-like column to a `datetime` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95df894",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f78082",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we can use the `dt` method (just like `.str.` for string ops) to get month, day, year, and whatever else we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5df511",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month_name()\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['date', 'day', 'month', 'year', 'day_of_week']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8eab1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.4.4 Complex string processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8458455d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how we might extract the bedrooms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d02877",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.beds_sqft.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d9aea",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def clean_bdrm(value):\n",
    "\n",
    "    if isinstance(value, str):\n",
    "        end = value.find('br')\n",
    "\n",
    "        if end == -1:\n",
    "            return\n",
    "        \n",
    "        else:\n",
    "            start = value.find('/') + 2\n",
    "            return int(value[start:end])\n",
    "\n",
    "    else:\n",
    "        return\n",
    "\n",
    "df['bedrooms'] = df['beds_sqft'].map(clean_bdrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e893181",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[['bedrooms', 'beds_sqft']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5707b47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And the same approach might work for creating a sqft column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad99fe",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def clean_sqft(value):\n",
    "\n",
    "    if isinstance(value, str):\n",
    "        end = value.find('ft')\n",
    "        \n",
    "        if end == -1:\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            if value.find('br') == -1:\n",
    "                start = value.find('/') + 2\n",
    "            else:\n",
    "                start = value.find('-') + 2\n",
    "\n",
    "            return int(value[start:end])\n",
    "\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd414daf",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df['sqft'] = df['beds_sqft'].map(clean_sqft)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6d624",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.5 Summarizing your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923dcae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's have a look at a statistical profile of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703328d",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d29550",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why are there different counts on the columns?\n",
    "- How do the price (rent) variable ranges look?  1 dollar in rent as min?  35,000 in rent as  maximum?\n",
    "- What about sqft?  1 sqft min and 12,700 sqft max?\n",
    "- You are now in the realm of real-world data, with **outliers**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3115e8bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.6 Dealing with outliers\n",
    "When we talk about **outliers**, we're not talking about the Malcom Gladwell kind. The kind of outliers we're talking about are the kind that are so far-fetched that they more likely represent bad data than real observations. And even if they are real, they're so amazingly rare that we don't want them to bias our analysis. In either case, we need to get rid of them.\n",
    "\n",
    "In the case of our Craigslist listings, we'll do this in three steps:\n",
    "1. Find outliers in rent, say the top and bottom 1%\n",
    "1. Analyze the data without missing data\n",
    "1. Create a dataset that removes the outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced5457",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.6.1 Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f2b2a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's get a quantile value at the 1st percentile to see the value that the top one percent of our records exceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3fb83",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "low = df['price'].dropna().quantile(.01)\n",
    "print(low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c50f675",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And now the top 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d407e2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "high = df['price'].dropna().quantile(.99)\n",
    "print(high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d2618",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://static01.nyt.com/images/2021/01/26/multimedia/26xp-photog/26xp-photog-superJumbo.jpg\" width=45%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481592a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's apply our filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c0166",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cleaned = df[(df['price'] < high) & (df['price'] > low)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca7b62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And maybe we can filter on # bedrooms, too. And why not drop rows with missing data while we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc287d64",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned[cleaned['bedrooms'] < 4].dropna()\n",
    "cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20412654",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.7 Continuous vs Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98cdb06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(13,5))\n",
    "cleaned['subregion'].value_counts(sort=False).plot(kind='bar', ax=axarr[0], title='Sub-region')\n",
    "axarr[0].set_ylabel(\"count\")\n",
    "cleaned['price'].plot(kind='kde', ax=axarr[1], title='Price ($)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231bca1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.7.1 Binning your data\n",
    "Sometimes you'll want to convert a continuous variable to categorical. pandas provides us with a few options for doing this:\n",
    "- `pd.cut()`: evenly _spaced_ bins, or define your own breaks\n",
    "- `pd.qcut()`: evenly _populated_ bins, or define your own percentile breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52f85e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.7.1.1 `pd.cut()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641288ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(cleaned['price'], 3, labels=['low', 'medium', 'high']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e46c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.cut(cleaned['price'], 3, labels=['low', 'medium', 'high']).value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1f026",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.cut(\n",
    "    cleaned['price'], [0, 1000, 5000, 20000],\n",
    "    labels=['low', 'medium', 'high']).value_counts(sort=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0d728",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.7.1.2 `pd.qcut()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1e083",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "quintile_df, bins = pd.qcut(\n",
    "    cleaned['price'], 5,\n",
    "    labels=['very low', 'low', 'average', 'high', 'very high'], retbins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c03a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade172d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.qcut(\n",
    "    cleaned['price'], [0, .1, .3, .7, .9, 1],\n",
    "    labels=['very low', 'low', 'medium', 'high', 'very high']).value_counts(sort=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb3b133",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.7.2 Dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7ff71",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sometimes you might want to do the opposite: convert a categorical variable to a continuous or numeric variable. The way to do this is to create \"dummy variables\", where each category becomes its own _column_ with values that equal 1 if the _row_ belongs to the category and 0 otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b23f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(cleaned['subregion']).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ff333",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can then merge your dummy columns back onto the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a6a80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cleaned.join(pd.get_dummies(cleaned['bedrooms'].astype(int), prefix='beds')).loc[:, 'bedrooms':].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969609ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.8 Putting it all together: Craigslist Rental Listings + SF1\n",
    "\n",
    "Let's load another set of rental listings. This one is a dataset I've already cleaned for you. I've also geocoded the addresses to get lat/lon coordinates and Census Block IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d759e",
   "metadata": {},
   "source": [
    "### 3.8.1 Merging data from two different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b14db7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rentals = pd.read_csv(\n",
    "    'data/sfbay_geocoded.csv',\n",
    "    usecols=['rent', 'bedrooms', 'sqft', 'fips_block', 'longitude', 'latitude'],\n",
    "    dtype={'fips_block': str}  # load fips_block as str, numeric type will drop leading zero\n",
    ")  \n",
    "rentals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c5174",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And merge it with the census data using the FIPS block codes, which are named differently in the two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0256c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rentals_sf1 = pd.merge(rentals, sf1, left_on='fips_block', right_on='blockfips')\n",
    "rentals_sf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d41a3",
   "metadata": {},
   "source": [
    "### 3.8.2 Multi-column group-by's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9a7d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can begin looking at this merged dataset.  Let's start by computing mean rents by county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a8bf6",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "county_rents = rentals_sf1.groupby(\n",
    "    rentals_sf1['county_name'])[['rent']].mean().sort_values(by='rent', ascending=False)\n",
    "county_rents.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09f905",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This result generally conforms to our expectations, with San Francisco having the highest average rent and Solano lowest. But what if Solano just has a higher percentage of 1 bedroom apartments? Could that bias our findings? How might we account for this possibility?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390a01d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One way we could investigate the effect of total bedrooms is to include bedrooms as an additional segmentation variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2755b62d",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rentals_sf1.groupby(['county_name', 'bedrooms'])['rent'].mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988719b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That bar chart is not ideal. Too small, and it would be nicer to make it separate colors for each number of bedrooms.  Also notice how the use of two groupby variables produces a MultiIndex, which makes for ugly axis labels at the very least. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a49dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use `unstack()` to convert one of the indices from row values to columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20021cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rentals_sf1.groupby(['county_name', 'bedrooms'])['rent'].mean().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8cc4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can plot a bar chart with the unstacked data, add a title, and set the figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844af383",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rentals_sf1.groupby(\n",
    "    ['county_name', 'bedrooms'])['rent'].mean().unstack().plot(\n",
    "    kind='bar', figsize=(14,6), title='Average Rents by County and # Bedrooms', ylabel='rent ($)', xlabel='county')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef3250",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that in one line of code we can filter, groupby, and plot our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99825e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:** What's wrong with the plot above? Is it really showing us what we're interested in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_sf1.groupby(\n",
    "    ['bedrooms', 'county_name'])['rent'].mean().unstack().plot(\n",
    "    kind='bar', figsize=(14,6), title='Average Rents by County and # Bedrooms', ylabel='rent ($)', xlabel='# bedrooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c01799",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.8.3 Pivot tables and Crosstabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b4f5f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember when I said that in programming there is always more than one way to skin a cat? Here is another way of skinning this one that should look familiar to all of you excel power users out there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46321176",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(rentals_sf1, values='rent', index=['bedrooms'], columns=['county_name']).plot(\n",
    "    kind='bar', figsize=(14,6), title='Average Rents by County and Bedrooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30516cff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `pivot_table()` function makes it easy to add also can add partial totals, or \"marginals\", to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1f5b0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(rentals_sf1, values='rent', index='county_name', columns='bedrooms', margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e600565",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Although mean is the default type of aggregation in pivot_table, you can use any aggregation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191f13d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(\n",
    "    rentals_sf1, values='rent', index='county_name', columns='bedrooms', aggfunc=\"count\", margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23e7fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We could use the `count` aggregation method to get a full frequency distribution or cross-tabulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60730e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But then again, there's an even simpler way to do this in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b7a2c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(rentals_sf1['county_name'], rentals_sf1['bedrooms'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14aa77f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And the `crosstab()` function comes with its own bells and whistles. For example, setting `normalize=True` will tells us the fraction of the region's total listings that are in each combination of county and number of bedrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268166a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(rentals_sf1['county_name'], rentals_sf1['bedrooms'], margins=True, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c43b02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could also normalize just the rows (index) or the just the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c424f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(rentals_sf1['county_name'], rentals_sf1['bedrooms'], margins=True, normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835bb61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(rentals_sf1['county_name'], rentals_sf1['bedrooms'], margins=True, normalize='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c247e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we want to look at more statistics than just mean? We can combine several aggregation methods and compute them at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8e818",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rentals_sf1[rentals_sf1['bedrooms'] < 4].groupby(\n",
    "    ['county_name', 'bedrooms'])['rent'].agg(['mean', 'std', 'min', 'max']).reset_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7011d01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.8.4 Exploring correlations in your data\n",
    "\n",
    "Pandas provides simple ways of computing correlation coefficients among the columns in your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40353f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rentals_sf1[['rent', 'sqft']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef28a8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And this method can be combined with groupby to compute correlation tables by group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9285e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rentals_sf1.groupby('county_name')[['rent', 'sqft']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce37b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_sf1.groupby('bedrooms')[['rent', 'sqft']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed45a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.8.5 Quantiles and User-defined Aggregation Functions\n",
    "\n",
    "Below the `cut()` function to create categories for ranges of a variable. In this example we use 4 even intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe151fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sqft_cat = pd.cut(rentals_sf1['sqft'], 4)\n",
    "sqft_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50026487",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's define our own aggregation function to get a standardized maximum rent for each sqft category. Standardization is the process of transforming your data such the the mean is 0 and the standard deviation is 1, which is accomplished by subtracting the mean and dividing by the standard deviation. By standardizing your data, you are able to make more generalized comparisons across groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b104cda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rentals_sf1.groupby(sqft_cat)['rent'].agg(\n",
    "    max_rent='max', standardized_max=lambda x: (x.max() - x.mean()) / x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0caa0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So even though the smallest apartment size category has the lowest maximum rent, it is the most _extreme_ maximum rent relative to its group mean!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48187d8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.9 Exercises\n",
    "\n",
    "Try practicing these techniques on your own, to do the following:\n",
    "\n",
    "* Calculate the mean sqft of rental listings by county\n",
    "* Calculate the standard deviation (std) of sqft of rental listings by county and bedroom\n",
    "* Add a new column with a normalized sqft, substracting the mean sqft by bedroom from each listing's sqft \n",
    "* Compute correlation coefficients among rent, sqft, pct_white, pct_black, pct_asian and pct_hisp, by county and for the region\n",
    "* Redo the statistical profile on rents by categories of sqft range using 10 quantiles rather than 4 equal bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb29a1",
   "metadata": {},
   "source": [
    "# 4. For Next Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd42d62",
   "metadata": {},
   "source": [
    "# 5 Questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
